# pr-04 spec — cache build-all builds primer sft + validates default caches

## goal

make `niels_gpt/cache/cli.py build-all` actually build **everything required by the repo defaults**, including the **primer SFT token cache**, and then **fail fast** if any required caches are missing/misconfigured.

this removes the “manual primer step” footgun and ensures a fresh clone can run `build-all` and then train with default configs.

## non-goals

- do not change training logic (optimizers, batching, loss, model)
- do not change tokenizer training artifacts or special token ids
- do not change dataset mixes/weights in configs (assume defaults already include primer in `mix_sft`)
- do not add new cache formats
- do not add new datasets beyond wiring primer into build-all

## assumptions (must hold after pr-03)

- primer SFT builder exists and writes to:
  - `data/cache/sft/primer/meta.json`
  - `data/cache/sft/primer/train/` and `.../val/` containing `shard_*.bin`
- primer raw file is JSONL at `data/primer.jsonl`
- build-all already builds:
  - pretrain caches: `fineweb_edu`, `roam`, `wikitext`, `gutenberg` (or stub)
  - sft caches: `dolly15k`, `oasst1`
- cache roots come from settings:
  - `settings.data.caches.pretrain_token_cache`
  - `settings.data.caches.sft_token_cache`

if any assumption is false, update code accordingly but **do not refactor**.

---

## required behavior

### 1) build-all builds primer sft cache

running:

```bash
python -m niels_gpt.cache.cli build-all

must build (or no-op if already built) all caches needed by default settings, including:
	•	sft/primer cache built from data/primer.jsonl

primer must be built unconditionally as part of build-all (because primer is in default mix_sft).

2) build-all ends with cache validation for defaults

after building, build-all must validate that caches required by default mixes exist on disk:
	•	pretrain: validate each key in settings.data.mix_pretrain
	•	existence of {pretrain_cache}/{src}/meta.json
	•	existence of {pretrain_cache}/{src}/{train,val}/
	•	sft: validate each key in settings.data.mix_sft
	•	existence of {sft_cache}/{src}/meta.json
	•	existence of {sft_cache}/{src}/{train,val}/

if anything is missing, raise FileNotFoundError with a clear multi-line list of missing paths.

also print a short “resolved plan” before building:
	•	which pretrain sources will be built
	•	which sft sources will be built
	•	which cache roots will be used

3) build-all is idempotent

running build-all twice should not error and should not rebuild caches unnecessarily (existing behavior is fine). it may print “already exists, skipping”.

⸻

api / interface constraints
	•	do not change the CLI command surface:
	•	command choices must remain exactly ["build-all"]
	•	only modify niels_gpt/cache/cli.py unless absolutely required
	•	do not import training modules into cache.cli (keep dependency direction clean)

⸻

implementation notes (do exactly this)

determine sources from settings

inside build_all(...), load settings and compute:
	•	pretrain_sources = list(settings.data.mix_pretrain.keys())
	•	sft_sources = list(settings.data.mix_sft.keys())

then:
	•	build pretrain caches only for pretrain_sources (subset of available builders)
	•	build sft caches only for sft_sources (subset of available builders)

primer must be included via sft_sources.

validation helper

add a small helper in cache/cli.py:

def _validate_token_cache_tree(cache_root: Path, sources: list[str]) -> None:
    ...

that checks for meta.json and train/val dirs and raises FileNotFoundError listing all missing.

call it at the end for both caches:
	•	_validate_token_cache_tree(Path(settings.data.caches.pretrain_token_cache), pretrain_sources)
	•	_validate_token_cache_tree(Path(settings.data.caches.sft_token_cache), sft_sources)

primer builder call

hook primer builder into the sft build section:
	•	read primer JSONL at data/primer.jsonl (use the same path the primer builder expects)
	•	ensure missing file produces a clear error: FileNotFoundError("missing primer jsonl: ...")

do not implement JSONL parsing here; call the existing builder from pr-03.

⸻

acceptance tests (must add)

create tests/test_cache_build_all.py with tests that do not download huge datasets.

use tmp_path to simulate cache directories and monkeypatch settings paths to point to temp dirs.

test 1: validates missing primer cache after build-all is skipped
	•	patch build_all to skip calling primer builder (simulate by temporarily monkeypatching the builder function to no-op)
	•	assert build-all raises FileNotFoundError listing sft/primer/meta.json and sft/primer/train/val

test 2: build-all calls primer builder
	•	monkeypatch the primer builder function to create the expected directory tree in temp sft cache
	•	run build-all
	•	assert sft/primer/meta.json exists

notes:
	•	you may need to structure builder imports so tests can monkeypatch the call site.
	•	tests must not run actual dataset downloads.

⸻

done means
	•	python -m niels_gpt.cache.cli build-all builds primer sft cache in addition to existing caches
	•	build-all ends by validating caches for default mixes and fails fast with clear missing-path errors
	•	build-all is idempotent
	•	tests pass (pytest)
